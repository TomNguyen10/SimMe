{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q  torch peft bitsandbytes transformers trl accelerate sentencepiece numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, PeftConfig\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "PROJECT_NAME = 'messages'\n",
    "RUN_NAME = 'a100'\n",
    "MODEL_NAME = f\"Tom10117/{PROJECT_NAME}-{RUN_NAME}\"\n",
    "MAX_LENGTH = 200\n",
    "ME = \"Tiến Dũng Nguyễn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    MODEL_NAME,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=MAX_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPRESS_TOKENS = [26308, 243, 162,   155,   149, 160, 47, 18610]\n",
    "BAD_WORDS = [[26308], [243], [162], [155], [149], [160], [47], [18610], [229,159,171]]\n",
    "\n",
    "\n",
    "def generate_next(text, min_tokens, max_tokens):\n",
    "  inputs, outputs = [], []\n",
    "  attempt = \"\"\n",
    "  final_tokens = []\n",
    "  try:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = model.generate(**inputs,\n",
    "                                    max_new_tokens=max_tokens,\n",
    "                                    min_new_tokens=min_tokens,\n",
    "                                    return_dict_in_generate=True,\n",
    "                                    output_scores=False,\n",
    "                                    no_repeat_ngram_size=6,\n",
    "                                    suppress_tokens = SUPPRESS_TOKENS,\n",
    "                                    bad_words_ids = BAD_WORDS)\n",
    "    sequence = outputs['sequences'][0]\n",
    "    attempt = tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "    final_tokens = sequence[:-10]\n",
    "  finally:\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "  return attempt, final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message:\n",
    "  def __init__(self, sender=None, message=None, text=None):\n",
    "    self.is_complete = True\n",
    "    if message is not None:\n",
    "      self.sender = ME if sender is None else sender\n",
    "      self.message = message\n",
    "    else:\n",
    "      if ':' not in text and ';' in text:\n",
    "        text = text.replace(';',':')\n",
    "      if ':' not in text:\n",
    "        self.sender = text\n",
    "        self.message = ''\n",
    "        self.is_complete = False\n",
    "      else:\n",
    "        beginning, ending = text.split(':')\n",
    "        self.sender = beginning.replace('###', '').strip()\n",
    "        self.message = ending.strip()\n",
    "\n",
    "  def __repr__(self):\n",
    "    if self.is_complete:\n",
    "      return f'### {self.sender}: {self.message}'\n",
    "    else:\n",
    "      return f'### {self.sender}'\n",
    "\n",
    "\n",
    "class Conversation:\n",
    "\n",
    "  NLI_MAX_MESSAGES = 20\n",
    "\n",
    "  def __init__(self, who):\n",
    "    self.who = who\n",
    "    self.messages = []\n",
    "    self.nli_message_count = 0\n",
    "    self.current_sender = ME\n",
    "\n",
    "  def prefix(self):\n",
    "    result = f\"<<SYS>>Write a realistic text message chat. Avoid repetition.<</SYS>>\\n\"\n",
    "    result += f\"[INST]Write a chat between {ME} and {self.who}[/INST]\\n\"\n",
    "    return result\n",
    "\n",
    "  def next_sender(self):\n",
    "    self.current_sender = self.who if self.current_sender == ME else ME\n",
    "\n",
    "  def add(self, message_contents):\n",
    "    self.add_message(Message(message=message_contents, sender=self.current_sender))\n",
    "\n",
    "  def add_message(self, message):\n",
    "    self.messages.append(message)\n",
    "\n",
    "  def add_prompt(self):\n",
    "    self.add('')\n",
    "\n",
    "  def nli(self):\n",
    "    result = self.prefix()\n",
    "    nlis = [message.__repr__() for message in self.messages[-Conversation.NLI_MAX_MESSAGES:]]\n",
    "    self.nli_message_count = len(nlis)\n",
    "    result += ' '.join(nlis)\n",
    "    return result\n",
    "\n",
    "  def __repr__(self):\n",
    "    result = \"\"\n",
    "    for message in self.messages:\n",
    "      result += message.__repr__() + '\\n'\n",
    "    return result\n",
    "\n",
    "  def process(self, language):\n",
    "    language = language.replace('?:',':').replace('::',':')\n",
    "    incoming = language.replace(' ###','###').split('### ')[1:]\n",
    "    self.messages = self.messages[:-1] # remove the last message\n",
    "    new_messages = incoming[self.nli_message_count-1:]\n",
    "    for index, new_message in enumerate(incoming[self.nli_message_count-1:]):\n",
    "      message = Message(text=new_message)\n",
    "      if message.sender != self.current_sender and index != 0:\n",
    "        return True\n",
    "      else:\n",
    "        self.add_message(message)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, input_text, max_tokens=100, min_tokens=5):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        min_new_tokens=min_tokens,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=False,\n",
    "        no_repeat_ngram_size=6,\n",
    "        suppress_tokens=SUPPRESS_TOKENS,\n",
    "        bad_words_ids=BAD_WORDS\n",
    "    )\n",
    "\n",
    "    sequences = output['sequences'][0]\n",
    "\n",
    "    generated_text = tokenizer.decode(sequences, skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "input_text = \"what is 1 + 1 equal to\"\n",
    "response = generate_response(model, tokenizer, input_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare generation from baseline and fine-tuned model\n",
    "def compare_models(baseline_model, fine_tuned_model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    # Baseline model output\n",
    "    baseline_output = baseline_model.generate(**inputs, max_new_tokens=100)\n",
    "    baseline_response = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Fine-tuned model output\n",
    "    fine_tuned_output = fine_tuned_model.generate(**inputs, max_new_tokens=100)\n",
    "    fine_tuned_response = tokenizer.decode(fine_tuned_output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Baseline Model Response:\\n\", baseline_response)\n",
    "    print(\"Fine-Tuned Model Response:\\n\", fine_tuned_response)\n",
    "\n",
    "text = \"what is 1 + 1 equal to\"\n",
    "compare_models(base_model, model, tokenizer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Who is the conversation with?')\n",
    "who = input()\n",
    "\n",
    "conversation = Conversation(who)\n",
    "while True:\n",
    "  print(f'{conversation.current_sender}: ')\n",
    "  reply = input()\n",
    "  if reply == 'stop':\n",
    "    break\n",
    "  elif reply != '':\n",
    "    conversation.add(reply)\n",
    "  else:\n",
    "    conversation.add_prompt()\n",
    "    ready = False\n",
    "    while not ready:\n",
    "      language, final_tokens = generate_next(conversation.nli(), 3, 8)\n",
    "      ready = conversation.process(language)\n",
    "      clear_output(wait=True)\n",
    "      print(conversation)\n",
    "      # print(final_tokens)\n",
    "  conversation.next_sender()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
